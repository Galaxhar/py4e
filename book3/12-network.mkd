Programas em rede
=========================

Enquanto vários dos exemplos no livro estão focados em ler arquivos
e procurar dados nesses arquivos, existem várias fontes de informação diferentes
quando consideramos a Internet.

Neste capítulo, fingiremos ser um navegador da internet e acessaremos
as páginas da internet utilizando HyperText Transfer Protocol (HTTP). Então 
iremos os dados dessa página e utilizá-los.

HyperText Transfer Protocol - HTTP
------------------------------------

O protocolo de rede que provê a internet é atualmente simples e
existem suportes nativos em Python chamados `soquetes` que
facilita o trabalho de fazer conexões na rede e recuperar dados por meio
dos soquetes no programa de Python.

Um soquete parece com um arquivo, exceto que um simples soquete provê
uma conexão de via dupla entre dois programas. É possível ler e escrever
com o mesmo soquete. Se algo for escrito por um soquete, isso é enviado à 
aplicação que está do outro lado do soquete. Se algo for lido de um soquete,
os dados captados foram enviados pela outra aplicação.

Porém, ao tentar ler os dados de um soquete em que a outra aplicação não
enviou nenhum dado, apenas sente e espere. Se os programas do soquete simplesmente
esperarem por dados sem enviar nada, eles esperarão por muito lempo, então é uma
parte importante dos programas que se comunicam pela Internet é ter um protocolo.

Um protocolo é um conjunto de regras que determinam quem vai primeiro, o que ele
tem que fazer, quais as respostas para essa mensagem, quem envia depois, e assim 
por diante. Em resumo, cada uma das aplicações num soquete estão dançando e 
certificando de que nenhum dos dois pise no pé do outro.

Existem vários documentos que descrevem esses protocolos de rede. O protocolo HTTP 
é descrito no documento abaixo:

<https://www.w3.org/Protocols/rfc2616/rfc2616.txt>

Esse é um complexo e longo documento de 176 páginas com vários detalhes. Se você o
achar interessante, sinta-se livre para ler todo. Mas se você der uma olhada por 
volta da página 36 do RFC2616, você encontrará a sintáxe para a solicitação *GET*.
Para solicitar um documento de um servidor da Internet, nos conectamos com o
servidor `www.pr4e.org` pela porta 80 e então enviamos uma linha da forma:

`GET http://data.pr4e.org/romeo.txt HTTP/1.0 `

Em que o segundo parâmetro é a página que estamos solicitando e ainda enviamos uma
linha em branco. O servidor responderá com uma informação de cabeçalho sobre o
documento e a linha em branco seguida do conteúdo do documento.

O navegador da internet mais simples do mundo
-----------------------------------------------

Talvez a forma mais fácil de mostrar como o protocolo HTTP funciona é
escrevendo um simples programa em Python que faz conexão com um servidor 
da internet e segue as regras do protocolo HTTP para solicitar um documento
e mostrar em tela o que o servidor enviou como reposta.

\VerbatimInput{../code3/socket1.py}

Primeiramente o programa faz conexão com a porta 80 no servidor 
[www.py4e.com](http://www.py4e.com). Enquanto nosso programa está sendo o
"navegador da internet", o protocolo HTTP diz que precimaos enviar o 
comando *GET* seguido de uma linha em branco. `\r\n` significa um
FDL (final de linha), então `\r\n\r\n` significa nada entre dois FDL em 
sequência. Isso é equivalente a uma linha em branco.

![A Socket Connection](height=2.0in@../images/socket)

Tendo enviado a linha em branco, nós escrevemos um laço que recebe dados 
em pedaços de 512 caracteres do soquete e mostra na tela até não haver
mais dados para ler (i.e., the recv() retorna uma string nula)

O programa produz a seguinte saída:

~~~~
HTTP/1.1 200 OK
Date: Wed, 11 Apr 2018 18:52:55 GMT
Server: Apache/2.4.7 (Ubuntu)
Last-Modified: Sat, 13 May 2017 11:22:22 GMT
ETag: "a7-54f6609245537"
Accept-Ranges: bytes
Content-Length: 167
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: text/plain

Mas, sutil! que luz ultrapassa aquela janela?
Esse é o leste, e Julieta é o sol.
Ascenda, honesto sol, e acabe com a invejosa lua,
Que já está doente e pálida com o luto,
~~~~

A saída começa com um cabeçalho que o servidor envia para descrever
o documento. Por exemplo, o cabeçalho `Content-Type` (tipo de conteúdo) 
indica que o documento é um texto simples (`text/plain`).

Após o servidor enviar o cabeçalho, ele adiciona uma linha em branco
para indicar o final do cabeçalho e então envia os dados do arquivo 
*romeo.txt*.

Esse exemplo mostra como fazer uma conexão de rede de baixo nível
com soquetes. Soquetes podem ser usados para comunicações com 
um servidor na internet ou de e-mail ou qualquer outro tipo de servidor. 
O necessário é encontrar o documento que descreve o protocolo e escrever 
um código que envie e receba os dados de acordo com o protocolo.

Entretanto, já que o protocolo mais comum é o HTTP, Python tem uma 
biblioteca, especificamente criada que suporta o protocolo HTTP para 
recuperação de documentos e dados pela internet.

Um dos requisitos para o uso de protocolo HTTP é a necessidade de 
enviar e receber dados como objetos em *bytes*, ao invés de *strings*. 
No exemplo seguinte, os métodos `encode()` e `decode()` convertem 
*strings* em  objetos em *bytes* e vice versa.

O exemplo seguinte utiliza a notação `b''` para especificar a variável 
que precisa ser armazenada como um objeto em *bytes*. `encode()` e
`b''` são equivalentes.

~~~~
>>> b'Hello world'
b'Hello world'
>>> 'Hello world'.encode()
b'Hello world'
~~~~

Recuperando uma imagem sobre http
-----------------------------

\index{urllib!image}
\index{image!jpg}
\index{jpg}

No exemplo acima, recuperamos um arquivo de texto de plano que tinha uma nova linha no arquivo e simplesmente copiamos os dados para a tela conforme o programa foi executado. Podemos usar um programa semelhante para recuperar uma imagem através do uso de http. em vez de copiar os dados para a tela como o programa é executado, nós acumulamos os dados em uma seqüência de caracteres, aparar os cabeçalhos e, em seguida, salvar os dados de imagem em um arquivo da seguinte maneira:

\VerbatimInput{../code3/urljpeg.py}

Quando o programa é executado, ele produz a seguinte saída:

~~~~
$ python urljpeg.py
5120 5120
5120 10240
4240 14480
5120 19600
...
5120 214000
3200 217200
5120 222320
5120 227440
3167 230607
Header length 393
HTTP/1.1 200 OK
Date: Wed, 11 Apr 2018 18:54:09 GMT
Server: Apache/2.4.7 (Ubuntu)
Last-Modified: Mon, 15 May 2017 12:27:40 GMT
ETag: "38342-54f8f2e5b6277"
Accept-Ranges: bytes
Content-Length: 230210
Vary: Accept-Encoding
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: image/jpeg
~~~~

Você pode ver que para este URL, o cabeçalho do content-Type indica que o corpo do documento é na imagem(`image/jpeg`). uma vez que o programa for concluído, você pode visualizar os dados da imagem abrindo o arquivo stuff. jpg em um visualizador de imagens.

Como o programa é executado, você pode ver que não obtemos 5120 caracteres cada vez que chamamos o método `recv()`. obtemos tantos caracteres como foram transferidos através da rede para nós pelo servidor Web no momento em que chamamos `recv()`. Neste exemplo, nós  obtemos tão poucos como 3200 caracteres cada vez que solicitar até 5120 caracteres de dados.

Seus resultados podem ser diferente dependendo de sua velocidade de rede. Observe também que na última chamada para `recv()` obtemos 3167 bytes, que é o fim do fluxo, e na próxima chamada para `recv()`obtemos um comprimento zero que nos diz que o serviço chamou `close()` em seu final do soquete e não há mais dados próximos.

\index{time}
\index{time.sleep} 

podemos retardar nossas sucessivas chamadas de `recv()` descomentando a chamada para o `time.sleep()`. Desta forma, esperamos um quarto de segundo após cada chamada para que o servidor pode "chegar à frente" de nós e enviar mais dados para nós antes de chamar `recv()` novamente. com o atraso, no lugar o programa executa como segue:

~~~~
$ python urljpeg.py
5120 5120
5120 10240
5120 15360
...
5120 225280
5120 230400
207 230607
Header length 393
HTTP/1.1 200 OK
Date: Wed, 11 Apr 2018 21:42:08 GMT
Server: Apache/2.4.7 (Ubuntu)
Last-Modified: Mon, 15 May 2017 12:27:40 GMT
ETag: "38342-54f8f2e5b6277"
Accept-Ranges: bytes
Content-Length: 230210
Vary: Accept-Encoding
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: image/jpeg
~~~~

Agora, além da primeira e última chamadas para `recv()`, agora temos 5120 caracteres cada vez que pedimos novos dados. 

Há como buffer entre o servidor que faz solicitações `send()` e nosso aplicativo fazendo solicitações`recv()`. Quando executamos o programa com o atraso no lugar, em algum momento o servidor pode encher o buffer no soquete e ser forçado a pausar até que o nosso programa começa a esvaziar o buffer. a pausa do aplicativo de envio ou o aplicativo de recebimento é chamado de "controle de fluxo"

\index{flow control} 

Recuperando páginas da Web com `urllib`
---------------------------------------------

Enquanto podemos enviar e receber dados manualmente por http usando a biblioteca de módulo, há uma maneira muito mais simples de executar essa tarefa comum em Python usando a biblioteca  `urllib`.

Usando `urllib`, você pode tratar a página da Web muito parecido com um arquivo. Você simplesmente indica qual página da Web você gostaria de recuperar e `urllib` lida com todos os detalhes do protocolo http e do cabeçalho. 

O código equivalente para ler o arquivo Romeo.txt da Web usando `urllib` é o seguinte:

\VerbatimInput{../code3/urllib1.py}

uma vez que a página da Web foi aberta com `urllib.urlopen`,podemos tratá-lo como um arquivo e lê-lo usando um `for`
loop.  

Quando o programa é executado, só vemos a saída do conteúdo do arquivo. Os cabeçalhos são enviados, mas o código `urllib` consome os cabeçalhos e só retorna os dados para nós.

~~~~
Mas suave do que a luz através de quebras de janela Yonder
É o leste e Juliet é o sol
Levante-se sol justo e matar a lua invejoso
Que já está doente e pálido de luto

~~~~

Como exemplo, podemos escrever um programa para recuperar os dados para
`romeo.txt` e calcular a frequência de cada palavra no arquivo da seguinte maneira: 

\VerbatimInput{../code3/urlwords.py}

Novamente, uma vez que abrimos a página Web, podemos lê-lo como um arquivo local.

Lendo Arquivos Binários Utilizando `urllib`
-----------------------------------

Às vezes, você deseja recuperar um arquivo não textual(ou binário), como por exemplo um arquivo de imagem ou vídeo. Os dados contidos nesses arquivos geralmente não são úteis para imprimir, mas você pode facilmente criar uma cópia em uma URL para um arquivo local no disco rígido usando `urllib`.

\index{arquivo binário}

O padrão é abrir a URL e usar `read` para fazer download de todo o conteúdo do documento em uma variável string(`img`) e, em seguida, armazenar estas informações em um arquivo local da seguinte maneira:

\VerbatimInput{../code3/curl1.py}

Este programa lê todos os dados de entrada de uma só vez, em toda a rede, e
armazena-o na variável `img` na memória principal do seu
computador. Logo após abre o arquivo `cover.jpg` e grava os dados
para o seu disco. O argumento `wb` para a função ` open () `abre um arquivo binário
apenas para escrever. Este programa funcionará se o tamanho do arquivo for menor que
o tamanho da memória do seu computador.

Entretanto, se este for um arquivo de áudio ou vídeo de um tamanho mais robusto, este programa poderá falhar
ou pelo menos execute de maneira devagar quando o computador ficar sem memória.
Para evitar a falta de memória, recuperamos os dados em blocos
(ou buffers) e, em seguida, armazenamos cada bloco no seu disco antes de recuperar
o bloco seguinte. Desta forma, o programa pode ler qualquer tamanho de arquivo sem
usando toda a memória que você tem disponível no seu computador.

\VerbatimInput{../code3/curl2.py}

Neste exemplo, lemos apenas 100.000 caracteres por vez e, em seguida,
escrevemos estes caracteres no arquivo `cover.jpg` antes
de retomar os próximos 100.000 caracteres de dados da web.

Este programa é executado da seguinte maneira:

~~~~
python curl2.py
230210 characters copied.
~~~~

Analisando HTML e Raspando a Web
---------------------------------

\index{web!raspando}
\index{analisando HTML}

Um dos usos mais comuns do recurso `urllib` no Python
é *raspar* a web. Raspagem da Web é quando escrevemos um
programa que finge ser um navegador da web e retorna páginas,
para em seguida examinar os dados nessas páginas à procura de padrões.

Tomando como exemplo, um mecanismo de pesquisa como o Google pode examinar a fonte de
uma página da web, extraindo os links para outras páginas, para assim recuperar estas
páginas, extração de links e assim por diante. Usando essa técnica, o Google
*spider* percorre quase todas as páginas da rede.

O Google também usa a frequência dos links nas páginas encontradas para uma
página específica como um indicador de medição de quão "importante" é uma página e quão alto
a página deverá aparecer nos resultados do mecanismo de pesquisa.

Analisando HTML Usando Expressões Regulares
--------------------------------------

Uma maneira simples de analisar o HTML é usar expressões regulares para procurar repetidamente
e extrair substrings que correspondam a um padrão determinado.

Aqui está uma página da web simples:

~~~~ {.html}
<h1>The First Page</h1>
<p>
If you like, you can switch to the
<a href="http://www.dr-chuck.com/page2.htm">
Second Page</a>.
</p>
~~~~

Podemos construir uma expressão regular bem concebida para combinar e extrair
os valores do link do texto acima, da seguinte maneira:

~~~~
href="http[s]?://.+?"
~~~~

Nossa expressão regular procura por strings que começam com
"href=\"http://" ou "href=\"https://", seguido por um ou mais caracteres (`.+?`),
seguido por outra citação dupla. O ponto de interrogação atrás do `[s]?` Indica
que deve-se procurar a string "http" seguida de zero ou um "s".

O ponto de interrogação adicionado ao `.+?` indica
que a equivalência deve ser feita de maneira "não gananciosa" em vez de
moda "gananciosa". Uma partida não gananciosa tenta encontrar o
*menor* string de correspondência possível e uma correspondência gananciosa
tenta encontrar a *maior* string possível de correspondência.

\index{gananciosa}
\index{nao-gananciosa}

Adicionamos parênteses à nossa expressão regular para indicar qual parte da
nossa string correspondente que gostaríamos de extrair e produzir o seguinte
programa:

\index{regex!parenteses}
\index{parenteses!expressao regular}

\VerbatimInput{../code3/urlregex.py}

A biblioteca `ssl` permite que este programa acesse sites da web que estritamente
utilizam HTTPS. O método `read` retorna o código-fonte HTML como um objeto de bytes
em vez de retornar um objeto HTTPResponse. O método de expressão regular `findall`
nos fornecerá uma lista de todas as strings que correspondem à nossa
expressão regular, retornando apenas o texto do link entre aspas duplas.

Quando executamos o programa e inserimos uma URL, obtemos a seguinte saída:

~~~~
Enter - https://docs.python.org
https://docs.python.org/3/index.html
https://www.python.org/
https://docs.python.org/3.8/
https://docs.python.org/3.7/
https://docs.python.org/3.5/
https://docs.python.org/2.7/
https://www.python.org/doc/versions/
https://www.python.org/dev/peps/
https://wiki.python.org/moin/BeginnersGuide
https://wiki.python.org/moin/PythonBooks
https://www.python.org/doc/av/
https://www.python.org/
https://www.python.org/psf/donations/
http://sphinx.pocoo.org/
~~~~

Expressões regulares funcionam muito bem quando seu HTML está bem formatado
e previsível. Mas como existem muitas páginas HTML "quebradas"
lá, uma solução usando apenas expressões regulares pode perder alguns
links válidos ou acabam com dados incorretos.

Isso pode ser resolvido usando uma robusta biblioteca de análise de HTML.

Análise de HTML usando BeautifulSoup
------------------------------------

\index{BeautifulSoup}

Mesmo que HTML se pareça com XML^[O formato XML é descrito no próximo capítulo.] e algumas páginas são realmente construídas para serem XML, a maioria das páginas HTML é geralmente quebrada de forma que causaria à um analista de XML a rejeição inteira da página por ter um formato inapropriado.

Há várias bibliotecas Python que podem te ajudar a analisar HTML e extrair dados das páginas.Cada uma das bibliotecas têm seus pontos fortes e fracos e você pode escolher uma baseado nas suas necessidades.

Por exemplo, vamos simplesmente analisar algumas entradas HTML e extrair links usando a biblioteca BeautifulSoup. BeautifulSoup tolera HTML altamente falho e ainda é muito simples extrair os dados que você precisa. Você pode baixar e instalar o BeautifulSoup em:

<https://pypi.python.org/pypi/beautifulsoup4>

Informações sobre a instalação do BeautifulSoup com a ferramenta pip do Python Package Index podem ser encontradas em:

<https://packaging.python.org/tutorials/installing-packages/>

Nós vamos usar `urllib` para ler a página e então usaremos `BeautifulSoup` para extrair os atributos `href` das tags âncora (`a`).

\index{BeautifulSoup}
\index{HTML}
\index{parsing!HTML}

\VerbatimInput{../code3/urllinks.py}

O programa solicita um endereço web, então abre a página web, lê os dados e os passa para o analisador BeautifulSoup e, em seguida, recupera todas as tags âncora e imprime o atributo href de cada tag.

Quando o programa é executado, ele produz a seguinte saída:

~~~~
Enter - https://docs.python.org
genindex.html
py-modindex.html
https://www.python.org/
#
whatsnew/3.6.html
whatsnew/index.html
tutorial/index.html
library/index.html
reference/index.html
using/index.html
howto/index.html
installing/index.html
distributing/index.html
extending/index.html
c-api/index.html
faq/index.html
py-modindex.html
genindex.html
glossary.html
search.html
contents.html
bugs.html
about.html
license.html
copyright.html
download.html
https://docs.python.org/3.8/
https://docs.python.org/3.7/
https://docs.python.org/3.5/
https://docs.python.org/2.7/
https://www.python.org/doc/versions/
https://www.python.org/dev/peps/
https://wiki.python.org/moin/BeginnersGuide
https://wiki.python.org/moin/PythonBooks
https://www.python.org/doc/av/
genindex.html
py-modindex.html
https://www.python.org/
#
copyright.html
https://www.python.org/psf/donations/
bugs.html
http://sphinx.pocoo.org/
~~~~

Essa lista é muito mais longa porque algumas tags âncora são caminhos relativos (e.g., tutorial/index.html) ou referências in-page (por exemplo, '#') que não incluem "http://" ou "https://", que era um requisito em nossa expressão regular.

Você também pode usar o BeautifulSoup para extrair várias partes de cada tag:

\VerbatimInput{../code3/urllink2.py}

~~~~
python urllink2.py
Enter - http://www.dr-chuck.com/page1.htm
TAG: <a href="http://www.dr-chuck.com/page2.htm">
Second Page</a>
URL: http://www.dr-chuck.com/page2.htm
Content: ['\nSecond Page']
Attrs: [('href', 'http://www.dr-chuck.com/page2.htm')]
~~~~

`html.parser` é o analisador de HTML incluído na biblioteca padrão do Pyhton 3. Informações sobre outros analisadores de HTML estão disponíveis em:

<http://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser>

Esses exemplos só começam a mostrar o poder do BeautifulSoup quando se trata de analisar HTML.

Seção bônus para usuários Unix / Linux
--------------------------------------

Se você tem um computador Linux, Unix, ou Macintosh, você provavelmente tem no seu sistema operacional comandos incorporados que recuperam tanto textos simples quanto arquivos binários usando o HTTP ou Protocolos de transferência de arquivos (FTP).Um Destes comandos é `curl`:

\index{curl}

~~~~ {.bash}
$ curl -O http://www.py4e.com/cover.jpg
~~~~

O comando `curl`é a abreviação para "copy URL" e, portanto, os dois exemplos listados anteriormente para recuperar arquivos binários com `urllib` são habilmente denominados `curl1.py` e `curl2.py` em www.py4e.com/code3, pois implementam uma funcionalidade semelhante ao comando `curl`. Há também um exemplo de programa `curl3.py` que faz essa tarefa de forma um pouco mais eficaz, caso você queira realmente usar esse padrão em um programa que está escrevendo.

Um outro comando que funciona de maneira muito similar é `wget`:

\index{wget}

~~~~ {.bash}
$ wget http://www.py4e.com/cover.jpg
~~~~

Ambos os comandos tornam a recuperação de páginas da Web e arquivos remotos uma tarefa simples.

Glossário
--------
aranha
:   O ato de ferramentas de pesquisa na web recuperarem uma página e então
    todas as outras relacionadas e assim sucessivamente até terem quase todas as páginas
    na Internet que são utilizadas para construir os seus índices de pesquisa.
\index{aranha}
BeautifulSoup
:   Uma biblioteca de Python usada analisar documentos HTML e extrair dados destes que compensam a maioria
    das imperfeições no HTML  que o navegador geralmente ignora. Você pode baixar o código do 
    BeautifulSoup em  [www.crummy.com](http://www.crummy.com).
\index{BeautifulSoup}

porta
:   Um número que geralmente indica com qual aplicação você está se comunicando quando
    faz uma conexão de soquete com um servidor. Como no exemplo, o tráfego da web usualmente
    utiliza a porta 80 enquanto o tráfego de email utiliza a porta 25.
\index{porta}

Raspar
:   Quando um programa pretende ser um navegador da web e recupera uma página e então olha para o conteúdo dela.
    Frequentemente programas estão seguindo links em uma página para achar a próxima e assim atravessar uma rede de páginas
    ou uma rede social.
\index{raspar}

soquete
:   Uma rede de comunicações entre duas aplicações que podem enviar e receber
    uma informação em qualquer direção.
\index{soquete}



Exercícios
---------

**Exercício 1: Altere o programa de soquete `socket1.py` pR pedir 
ao usuário a URL para que então possa ler qualquer página da web.Você
pode usar `split('/')` para quebrar a URL em suas componentes para que
então possa extrair o nome do hospedeiro para que o soquete `connect` chame.
Adicione tratamento de erro usando `try` e `except` para lidar com a condição
do usuário digitar uma URL formatada incorretamente ou uma não existente**

**Exercício 2: Altere seu programa de soquete para que ele conte o número de
caracteres que recebeu e pare de mostrar qualquer texto depois que mostrar 3000
caracteres. O programa deve recuperar o documento inteiro e contar o número total de caracteres
e mostrar o resultado da contagem no final do documento.**

**Exercício 3: Use `urllib` para replicar o exercício anterior (1) 
recuperando um documento de uma URL, (2) mostrando até 3000 caracteres e (3) contando o total destes no documento.
Não se preocupe sobre os cabeçalhos para esse exercício, apenas mostre os 3000 primeiros caracteres
do contepudo do documento.**

**Exercício 4: Altere o programa `urllinks.py` para extrair e contar as _tags_ de parágrafos (p) do documento de HTML
recuperado e mostrar a contagem dos parágrafos como uma saída do seu programa.Não mostre o conteúdo, apenas conte-os.
Teste seu programa em várias páginas da web pequenas e também em algumas maiores.**

**Exercício 5: (Avançado) Altere o programa de soquete para que ele apenas mostre uma informação depois
dos cabeçalhos e uma linha em branco ser recebida. Lembre que `recv` recebe caracteres (_newlines_ e etc), não linhas.**

